{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89c84b90-f7c2-408e-adb3-1f346bd47051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Poster # / Board #                                       Poster title  \\\n",
      "0    PSTR034.01 / J6  Expression of senescence-associated Î²-galactos...   \n",
      "\n",
      "                         Full Author List             Date               Time  \\\n",
      "0  T. KOMORI1, E. KURIYAMA2, Y. MORIKAWA1  October 5, 2024  1:00 PM - 5:00 PM   \n",
      "\n",
      "     Location Session #                Session Type  \\\n",
      "0  MCP Hall A   PSTR034  Cellular Actions of Stress   \n",
      "\n",
      "                                        Affliliation Senior Author/PI Account  \n",
      "0  1Dept. of Anat. & Neurobio., 2Dept. of Neurolo...             Y. MORIKAWA1  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Configure Chrome options\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "# chrome_options.add_argument(\"--incognito\")\n",
    "# chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.binary_location = \"/usr/bin/google-chrome\"\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "# chrome_options.add_argument(\n",
    "#     \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "# )\n",
    "chromedriver_path = \"/usr/local/bin/chromedriver\"\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "def create_webdriver():\n",
    "    \"\"\"\n",
    "    Create a new instance of the Chrome WebDriver with specified options.\n",
    "    \"\"\"\n",
    "    return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# def extract_main_heading(browser):\n",
    "#     \"\"\"\n",
    "#     Extract the main heading from the page.\n",
    "\n",
    "#     Args:\n",
    "#         browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing text before '/', between '/' and '-', and after '-'.\n",
    "#     \"\"\"\n",
    "#     main_heading_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[1]/div/div[3]/div[1]/h1')\n",
    "#     main_heading_text = main_heading_element.text\n",
    "#     heading_pattern = r'^(.*?) / (.*?) - (.*)$'\n",
    "#     heading_match = re.search(heading_pattern, main_heading_text)\n",
    "    \n",
    "#     if heading_match:\n",
    "#         text_before_slash = heading_match.group(1)\n",
    "#         text_between_slash_and_dash = heading_match.group(2)\n",
    "#         text_after_dash = heading_match.group(3)\n",
    "#         return text_before_slash, text_between_slash_and_dash, text_after_dash\n",
    "#     return None, None, None\n",
    "\n",
    "def extract_main_heading(browser):\n",
    "    \"\"\"\n",
    "    Extract the main heading from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the text before and after the dash.\n",
    "    \"\"\"\n",
    "    main_heading_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[1]/div/div[3]/div[1]/h1')\n",
    "    main_heading_text = main_heading_element.text\n",
    "    heading_pattern = r'^(.*?) / (.*?) - (.*)$'\n",
    "    heading_match = re.search(heading_pattern, main_heading_text)\n",
    "    \n",
    "    if heading_match:\n",
    "        text_before_dash = heading_match.group(1) + ' / ' + heading_match.group(2)\n",
    "        text_after_dash = heading_match.group(3)\n",
    "        return text_before_dash, text_after_dash\n",
    "    return None, None\n",
    "    \n",
    "def extract_date_time(browser):\n",
    "    \"\"\"\n",
    "    Extract the date and time range from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the date and time range.\n",
    "    \"\"\"\n",
    "    date_time_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[1]/div/table/tbody/tr/td[1]/span')\n",
    "    date_time_text = date_time_element.text\n",
    "    date_time_pattern = r\"([a-zA-Z]+\\s\\d{1,2},\\s\\d{4}),\\s(.+)\"\n",
    "    date_time_match = re.search(date_time_pattern, date_time_text)\n",
    "    \n",
    "    if date_time_match:\n",
    "        date = date_time_match.group(1)\n",
    "        time_range = date_time_match.group(2)\n",
    "        return date, time_range\n",
    "    return None, None\n",
    "\n",
    "def extract_authors_address(browser):\n",
    "    \"\"\"\n",
    "    Extract the first author, last author, and address from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the first author, last author, and address.\n",
    "    \"\"\"\n",
    "    authors_address_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[2]/div[2]/dl/dd[1]')\n",
    "    authors_address_text = authors_address_element.text\n",
    "    authors_address_pattern = r\"\\*(.*?);\\s(.*)\"\n",
    "    authors_address_match = re.search(authors_address_pattern, authors_address_text)\n",
    "    \n",
    "    if authors_address_match:\n",
    "        authors_string = authors_address_match.group(1)\n",
    "        address = authors_address_match.group(2)\n",
    "        authors_list = [author.strip() for author in authors_string.split(',')]\n",
    "        first_author = authors_list[0]\n",
    "        last_author = authors_list[-1]\n",
    "        return authors_string, last_author, address\n",
    "    return None, None, None\n",
    "\n",
    "def extract_abstract(browser):\n",
    "    \"\"\"\n",
    "    Extract the abstract from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        str: The abstract text.\n",
    "    \"\"\"\n",
    "    abstract_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[2]/div[2]/dl/dd[3]')\n",
    "    return abstract_element.text\n",
    "\n",
    "def extract_location(browser):\n",
    "    \"\"\"\n",
    "    Extract the location information from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        str: The location text.\n",
    "    \"\"\"\n",
    "    location_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[1]/div/table/tbody/tr/td[2]/span')\n",
    "    return location_element.text\n",
    "\n",
    "def extract_poster_type(browser):\n",
    "    \"\"\"\n",
    "    Extract the poster type from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        str: The poster type text.\n",
    "    \"\"\"\n",
    "    poster_type_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[2]/div[1]/div/dl/dd[2]')\n",
    "    return poster_type_element.text\n",
    "\n",
    "def extract_category_type(browser):\n",
    "    \"\"\"\n",
    "    Extract the poster type from the page.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "\n",
    "    Returns:\n",
    "        str: The poster type text.\n",
    "    \"\"\"\n",
    "    poster_type_element = browser.find_element(By.XPATH, '//*[@id=\"body\"]/div/div[1]/div/div[3]/div[1]/h2/a')\n",
    "    heading_pattern = r'^Session (\\S+) - (.*)$'\n",
    "    heading_match = re.search(heading_pattern, poster_type_element.text)\n",
    "    \n",
    "    if heading_match:\n",
    "        session_number = heading_match.group(1)\n",
    "        text_after_dash = heading_match.group(2)\n",
    "        return session_number, text_after_dash\n",
    "    return None, None\n",
    "\n",
    "\n",
    "\n",
    "def reload_and_get_content(browser, url, url_id, max_retries=5):\n",
    "    \"\"\"\n",
    "    Reload the page and extract information with retries.\n",
    "\n",
    "    Args:\n",
    "        browser (webdriver.Chrome): The WebDriver instance.\n",
    "        url (str): The URL of the page to scrape.\n",
    "        url_id (int): The ID or index of the URL.\n",
    "        max_retries (int): The maximum number of retry attempts.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing extracted data, or None if all retries fail.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        browser.get(url)\n",
    "        try:\n",
    "            element_present = EC.presence_of_element_located((By.XPATH, f'//*[contains(text(), \"Add to Itinerary\")]'))\n",
    "            wait = WebDriverWait(browser, timeout=12, poll_frequency=3)\n",
    "            wait.until(element_present)\n",
    "\n",
    "            # Extract information from the page\n",
    "            # text_before_slash, text_between_slash_and_dash, text_after_dash = extract_main_heading(browser)\n",
    "            text_before_dash, text_after_dash = extract_main_heading(browser)\n",
    "\n",
    "            date, time_range = extract_date_time(browser)\n",
    "            first_author, last_author, address = extract_authors_address(browser)\n",
    "            abstract_text = extract_abstract(browser)\n",
    "            location = extract_location(browser)\n",
    "            poster_type = extract_poster_type(browser)\n",
    "            session_number, category_type = extract_category_type(browser)\n",
    "\n",
    "            return {\n",
    "                \"Poster # / Board #\": text_before_dash,\n",
    "                \"Poster title\": text_after_dash,\n",
    "                \"Full Author List\": first_author,             \n",
    "                \"Date\": date,\n",
    "                \"Time\": time_range,\n",
    "                \"Location\": location,\n",
    "                \"Session #\": session_number,               \n",
    "                \"Session Type\": category_type,\n",
    "                \"Affliliation\": address,                \n",
    "                \"Senior Author/PI Account\": last_author,\n",
    "                # \"abstract\": abstract_text,\n",
    "                # \"poster_type\": poster_type,\n",
    "                \"url\": url,\n",
    "            }\n",
    "        \n",
    "        except TimeoutException:\n",
    "            # Log the retry attempt and wait before retrying\n",
    "            print(f\"TimeoutException: Retry attempt {attempt + 1}\")\n",
    "            browser.refresh()\n",
    "            time.sleep(3)\n",
    "\n",
    "    # Log failure if all retries fail\n",
    "    print(f\"Failed to retrieve content from {url} after {max_retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# Read the URLs from CSV\n",
    "urls_df = pd.read_csv(r'urls.csv')\n",
    "all_results = []\n",
    "\n",
    "# Process each URL\n",
    "for index, row in urls_df.iterrows():\n",
    "    url = row['url']\n",
    "    browser = create_webdriver()\n",
    "    \n",
    "    result = reload_and_get_content(browser, url, index)\n",
    "    if result:\n",
    "        all_results.append(result)\n",
    "    browser.quit()\n",
    "\n",
    "# Save results to CSV\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv('output_data.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the results\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870cee43-a550-4419-bf3c-2be2371f297f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea65893-cd5f-467d-81c8-40280b86f3da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
